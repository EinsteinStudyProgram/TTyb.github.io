<!DOCTYPE html>
<html>

<head>

    <link rel="shortcut icon" href="/static/jpg/myjpg.jpg"/>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">

    <!-- Stylesheets -->
    <link href="/static/css/bootstrap.min.css" rel="stylesheet"/>
    <link href="/static/css/font-awesome.min.css" rel="stylesheet"/>
	<link href="/static/css/styles.css" rel="stylesheet"/>

    <script type="text/javascript" src="/static/js/jquery.min.js"></script>
    <script type="text/javascript" src="/static/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="/static/js/modernizr.js"></script>
    <script type="text/javascript" src="/static/js/prefixfree.min.js"></script>
    <title>百哥么么哒|个人网站</title>

</head>


<body>
    <div class="navbar navbar-default navbar-fixed-top">
    <div class="container">
        <a class="navbar-brand brand" href="/">TTyb|个人网站</a>
        <div style="font-size: 6px;font-family: serif;text-decoration: none;">
            <ul class="nav navbar-nav">
                <li><a class="navbar-brand" href="/">首页</a></li>
                <li><a class="navbar-brand" href="/topic">文章</a></li>
                <li><a class="navbar-brand" href="/category">分类</a></li>
                <li><a class="navbar-brand" href="/log/log">日志</a></li>
                <li><a class="navbar-brand" href="/guestbook">留言</a></li>
            </ul>
        </div>

        <div class="pull-right">
            <div style="font-size: 4px;font-family: serif;">
                <ul class="nav navbar-nav">
                    <li>
                        <a href="/contact" class="navbar-brand">联系方式</a>
                    </li>
                    <li>
                        <a data-toggle="dropdown" class="dropdown-toggle navbar-brand" href="#">友情链接<strong
                                class="caret"></strong></a>
                        <ul class="dropdown-menu container-fluid" style="border-radius: 6px 6px 6px 6px;">
                            <li><a href="http://www.cnblogs.com/TTyb">TTyb博客园</a></li>
                            <li><a href="http://www.cnblogs.com/nima">一只尼玛博客园</a></li>
                            <li><a href="https://github.com/TTyb">TTybGithub</a></li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

    </div>
</div>

        <div class="container">
    <div class="page-header">
        <hr>
        <h3>java从零到变身爬虫大神（一）</a></h3>
        <small>
            分类：java
        </small>
        <h6 class="text-muted">作者:TTyb&nbsp;&nbsp;文章发表于 2016-08-18</h6>
        <br/>
        <p class="content text-overflow">
            <p>学习java3天有余，知道一些基本语法后</p>

<p>学习java爬虫，1天后开始出现明显效果</p>

<p>刚开始先从最简单的爬虫逻辑入手</p>

<p>爬虫最简单的解析面真的是这样</p>

<div class="highlighter-rouge"><pre class="highlight"><code>import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import java.io.IOException;

public class Test {
    public static void Get_Url(String url) {
        try {
         Document doc = Jsoup.connect(url) 
          //.data("query", "Java")
          //.userAgent("头部")
          //.cookie("auth", "token")
          //.timeout(3000)
          //.post()
          .get();
        } catch (IOException e) {
              e.printStackTrace();
        }
    }
}
</code></pre>
</div>

<p>这只是一个函数而已</p>

<p>那么在下面加上：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>//main函数
     public static void main(String[] args) {
         String url = "...";
         Get_Url(url);
     }
</code></pre>
</div>

<p>哈哈，搞定</p>

<p>就是这么一个爬虫了</p>

<p>太神奇</p>

<p>但是得到的只是网页的html页面的东西</p>

<p>而且还没筛选</p>

<p>那么就筛选吧</p>

<div class="highlighter-rouge"><pre class="highlight"><code>public static void Get_Url(String url) {
        try {
         Document doc = Jsoup.connect(url) 
          //.data("query", "Java")
          //.userAgent("头部")
          //.cookie("auth", "token")
          //.timeout(3000)
          //.post()
          .get();
         
        //得到html的所有东西
        Element content = doc.getElementById("content");
        //分离出html下&lt;a&gt;...&lt;/a&gt;之间的所有东西
        Elements links = content.getElementsByTag("a");
        //Elements links = doc.select("a[href]");
        // 扩展名为.png的图片
        Elements pngs = doc.select("img[src$=.png]");
        // class等于masthead的div标签
        Element masthead = doc.select("div.masthead").first();
            
        for (Element link : links) {
              //得到&lt;a&gt;...&lt;/a&gt;里面的网址
              String linkHref = link.attr("href");
              //得到&lt;a&gt;...&lt;/a&gt;里面的汉字
              String linkText = link.text();
              System.out.println(linkText);
            }
        } catch (IOException e) {
              e.printStackTrace();
        }
    }
</code></pre>
</div>

<p>那就用上面的来解析一下我的博客园</p>

<p>解析的是<a>…</a>之间的东西</p>

<p><img src="http://images2015.cnblogs.com/blog/996148/201608/996148-20160818163033000-908471592.jpg" alt="" /></p>

<p>看起来很不错，就是不错</p>

<p>——————————-我是快乐的分割线——————————-</p>

<p>其实还有另外一种爬虫的方法更加好</p>

<p>他能批量爬取网页保存到本地</p>

<p>先保存在本地再去正则什么的筛选自己想要的东西</p>

<p>这样效率比上面的那个高了很多</p>

<p>很多</p>

<p>很多</p>

<p>看代码！</p>

<div class="highlighter-rouge"><pre class="highlight"><code>//将抓取的网页变成html文件，保存在本地
    public static void Save_Html(String url) {
        try {
            File dest = new File("src/temp_html/" + "保存的html的名字.html");
            //接收字节输入流
            InputStream is;
            //字节输出流
            FileOutputStream fos = new FileOutputStream(dest);
    
            URL temp = new URL(url);
            is = temp.openStream();
            
            //为字节输入流加缓冲
            BufferedInputStream bis = new BufferedInputStream(is);
            //为字节输出流加缓冲
            BufferedOutputStream bos = new BufferedOutputStream(fos);
    
            int length;
    
            byte[] bytes = new byte[1024*20];
            while((length = bis.read(bytes, 0, bytes.length)) != -1){
                fos.write(bytes, 0, length);
            }

            bos.close();
            fos.close();
            bis.close();
            is.close();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
</code></pre>
</div>

<p>这个方法直接将html保存在了文件夹src/temp_html/里面</p>

<p>在批量抓取网页的时候</p>

<p>都是先抓下来，保存为html或者json</p>

<p>然后在正则什么的进数据库</p>

<p>东西在本地了，自己想怎么搞就怎么搞</p>

<p>反爬虫关我什么事</p>

<p>上面两个方法都会造成一个问题</p>

<p><img src="http://images2015.cnblogs.com/blog/996148/201608/996148-20160818163928796-2014980506.jpg" alt="" /></p>

<p>这个错误代表</p>

<p>这种爬虫方法太low逼</p>

<p>大部分网页都禁止了</p>

<p>所以，要加个头</p>

<p>就是UA</p>

<p>方法一那里的头部那里直接</p>

<p><code class="highlighter-rouge">userAgent("Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0; MALC)")</code></p>

<p>方法二间接加：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>URL temp = new URL(url);
URLConnection uc = temp.openConnection();
uc.addRequestProperty("User-Agent", "Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5");
is = temp.openStream();
</code></pre>
</div>

<p>加了头部，几乎可以应付大部分网址了</p>

<p>——————————-我是快乐的分割线——————————-</p>

<p>将html下载到本地后需要解析啊</p>

<p>解析啊看这里啊</p>

<div class="highlighter-rouge"><pre class="highlight"><code>//解析本地的html
    public static void Get_Localhtml(String path) {

        //读取本地html的路径
        File file = new File(path);
        //生成一个数组用来存储这些路径下的文件名
        File[] array = file.listFiles();
        //写个循环读取这些文件的名字
        
        for(int i=0;i&lt;array.length;i++){
            try{
                if(array[i].isFile()){
                //文件名字
                System.out.println("正在解析网址：" + array[i].getName());

                //下面开始解析本地的html
                Document doc = Jsoup.parse(array[i], "UTF-8");
                //得到html的所有东西
                Element content = doc.getElementById("content");
                //分离出html下&lt;a&gt;...&lt;/a&gt;之间的所有东西
                Elements links = content.getElementsByTag("a");
                //Elements links = doc.select("a[href]");
                // 扩展名为.png的图片
                Elements pngs = doc.select("img[src$=.png]");
                // class等于masthead的div标签
                Element masthead = doc.select("div.masthead").first();
                
                for (Element link : links) {
                      //得到&lt;a&gt;...&lt;/a&gt;里面的网址
                      String linkHref = link.attr("href");
                      //得到&lt;a&gt;...&lt;/a&gt;里面的汉字
                      String linkText = link.text();
                      System.out.println(linkText);
                        }
                    }
                }catch (Exception e) {
                    System.out.println("网址：" + array[i].getName() + "解析出错");
                    e.printStackTrace();
                    continue;
                }
        }
    }
</code></pre>
</div>

<p>文字配的很漂亮</p>

<p>就这样解析出来啦</p>

<p>主函数加上</p>

<div class="highlighter-rouge"><pre class="highlight"><code>//main函数
    public static void main(String[] args) {
        String url = "http://www.cnblogs.com/TTyb/";
        String path = "src/temp_html/";
        Get_Localhtml(path);
    }
</code></pre>
</div>

<p>那么这个文件夹里面的所有的html都要被我解析掉</p>

<p>好啦</p>

<p>3天java1天爬虫的结果就是这样子咯</p>

<p>——————————-我是快乐的分割线——————————-</p>

<p>其实对于这两种爬取html的方法来说，最好结合在一起</p>

<p>作者测试过</p>

<p>方法二稳定性不足</p>

<p>方法一速度不好</p>

<p>所以自己改正</p>

<p>将方法一放到方法二的catch里面去</p>

<p>当方法二出现错误的时候就会用到方法一</p>

<p>但是当方法一也错误的时候就跳过吧</p>

<p>结合如下：</p>

<div class="highlighter-rouge"><pre class="highlight"><code>import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.BufferedReader;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.net.HttpURLConnection;
import java.net.URL;
import java.net.URLConnection;
import java.util.Date;
import java.text.SimpleDateFormat;

public class JavaSpider {
    
    //将抓取的网页变成html文件，保存在本地
    public static void Save_Html(String url) {
        try {
            File dest = new File("src/temp_html/" + "我是名字.html");
            //接收字节输入流
            InputStream is;
            //字节输出流
            FileOutputStream fos = new FileOutputStream(dest);
    
            URL temp = new URL(url);
            URLConnection uc = temp.openConnection();
            uc.addRequestProperty("User-Agent", "Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5");
            is = temp.openStream();
            
            //为字节输入流加缓冲
            BufferedInputStream bis = new BufferedInputStream(is);
            //为字节输出流加缓冲
            BufferedOutputStream bos = new BufferedOutputStream(fos);
    
            int length;
    
            byte[] bytes = new byte[1024*20];
            while((length = bis.read(bytes, 0, bytes.length)) != -1){
                fos.write(bytes, 0, length);
            }

            bos.close();
            fos.close();
            bis.close();
            is.close();
        } catch (IOException e) {
            e.printStackTrace();
            System.out.println("openStream流错误，跳转get流");
            //如果上面的那种方法解析错误
            //那么就用下面这一种方法解析
            try{
                Document doc = Jsoup.connect(url)
                .userAgent("Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0; MALC)")
                .timeout(3000) 
                .get();
                
                File dest = new File("src/temp_html/" + "我是名字.html");
                if(!dest.exists())
                    dest.createNewFile();
                FileOutputStream out=new FileOutputStream(dest,false);
                out.write(doc.toString().getBytes("utf-8"));
                out.close();

            }catch (IOException E) {
                E.printStackTrace();
                System.out.println("get流错误，请检查网址是否正确");
            }
            
        }
    }
    
    //解析本地的html
    public static void Get_Localhtml(String path) {

        //读取本地html的路径
        File file = new File(path);
        //生成一个数组用来存储这些路径下的文件名
        File[] array = file.listFiles();
        //写个循环读取这些文件的名字
        
        for(int i=0;i&lt;array.length;i++){
            try{
                if(array[i].isFile()){
                    //文件名字
                    System.out.println("正在解析网址：" + array[i].getName());
                    //文件地址加文件名字
                    //System.out.println("#####" + array[i]); 
                    //一样的文件地址加文件名字
                    //System.out.println("*****" + array[i].getPath()); 
                    
                    
                    //下面开始解析本地的html
                    Document doc = Jsoup.parse(array[i], "UTF-8");
                    //得到html的所有东西
                    Element content = doc.getElementById("content");
                    //分离出html下&lt;a&gt;...&lt;/a&gt;之间的所有东西
                    Elements links = content.getElementsByTag("a");
                    //Elements links = doc.select("a[href]");
                    // 扩展名为.png的图片
                    Elements pngs = doc.select("img[src$=.png]");
                    // class等于masthead的div标签
                    Element masthead = doc.select("div.masthead").first();
                    
                    for (Element link : links) {
                          //得到&lt;a&gt;...&lt;/a&gt;里面的网址
                          String linkHref = link.attr("href");
                          //得到&lt;a&gt;...&lt;/a&gt;里面的汉字
                          String linkText = link.text();
                          System.out.println(linkText);
                        }
                    }
                }catch (Exception e) {
                    System.out.println("网址：" + array[i].getName() + "解析出错");
                    e.printStackTrace();
                    continue;
                }
            }
        }
    //main函数
    public static void main(String[] args) {
        String url = "http://www.cnblogs.com/TTyb/";
        String path = "src/temp_html/";
        //保存到本地的网页地址
        Save_Html(url);
        //解析本地的网页地址
        Get_Localhtml(path);
    }
}
</code></pre>
</div>

<p>总的来说</p>

<p>java爬虫的方法比python的多好多</p>

<p>java的库真特么变态</p>

        </p>
    </div>
    <!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="http://localhost:4000/java/java%E4%BB%8E%E9%9B%B6%E5%88%B0%E5%8F%98%E8%BA%AB%E7%88%AC%E8%99%AB%E5%A4%A7%E7%A5%9E-%E4%B8%80.html"
     data-title="java从零到变身爬虫大神（一）" data-url="http://localhost:4000/java/java%E4%BB%8E%E9%9B%B6%E5%88%B0%E5%8F%98%E8%BA%AB%E7%88%AC%E8%99%AB%E5%A4%A7%E7%A5%9E-%E4%B8%80.html"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"ttyb"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0]
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();

</script>
<!-- 多说公共JS代码 end -->

</div>


    
<a class="gototop"></a>
<div class="container">
    <div id="footer" class="text-muted">
        <div style="font-family: 'Lato', sans-serif;">
            <div class="foo">
                <span class="letter" data-letter="T">T</span>
                <span class="letter" data-letter="T">T</span>
                <span class="letter" data-letter="y">y</span>
                <span class="letter" data-letter="b">b</span>
            </div>
        </div>
        <h3 style="font-size: 1.2em;font-family: KaiTi;">无聊就想打码 打码使我快乐</h3>
        <div style="font-size: 1.2em;font-family: KaiTi;">
            不用多久<br/>
            我就会升职加薪<br/>
            当上总经理<br/>
            出任CEO<br/>
            迎娶白富美<br/>
            走上人生巅峰
        </div>
    </div>
</div>
<footer id="footer">
    <ul class="icons">
        <li><a href="/contact" class="icon fa-qq"><span class="label">QQ</span></a></li>
        <li><a href="https://github.com/TTyb" class="icon fa-github"><span class="label">Github</span></a></li>
        <li><a href="/" class="icon fa-home"><span class="label">首页</span></a></li>
        <li><a href="http://www.cnblogs.com/TTyb/" class="icon fa-heart"><span class="label">博客园</span></a></li>
        <li><a href="/404" class="icon fa-map-signs"><span class="label">404</span></a></li>
    </ul>
    <p class="copyright">Copyright &copy; TTyb All rights reserved.</p>
</footer>

<script type="text/javascript" src="/static/js/jquery.gototop.min.js"></script>
<script type="text/javascript">
    $(function(){
            // $(".gototop").gototop();
            $(".gototop").gototop({
                position : 0,
                duration : 1250,
                visibleAt : 300,
                classname : "isvisible"
            });
        });
</script>


</body>
</html>
