---
layout: post
categories: [crawler]
title: 验证码
date: 2117-06-11
author: TTyb
desc: "11.验证码"
showdate: 2017-06-11
---

在爬虫的过程中，可能都会遇到一个很头疼的问题： **验证码** 。弹出验证码是 **反爬虫** 的一种方式，主要是由于这些原因：

>1. 抓取频率太高
>2. 爬虫未伪装
>3. 网站限制访问次数

对于第一种情况，如果爬虫的要求不是很高的话，或者网站很奇葩，一定需要时间间隔，例如：

<p style="text-align:center"><img  src="/img/crawler11/result1.jpg"/></p>

在抓取这类网站的时候，可以在每个抓取周期里面加上一个暂停，每次抓取都暂停 `3` 秒：

```
import time
time.sleep(3)
```

一般就可以杜绝这种情况的发生了。什么时候才算是爬虫的要求不高？爬虫可以分为两类：

>1. 主动式爬虫
>2. 被动式爬虫

主动式爬虫指的是，输入一个关键词，然后根据这个关键词去抓取相关的信息下来。这类的爬虫一般要求的数据量少，对爬虫要求没那么高，一般的爬虫工作者完全能胜任此类要求。

被动式爬虫指的是，实时监测网站更新的内容，一旦发现更新就将其抓取下来，这类一般出现在新闻公司。还有一种是要求将网站某一种类的信息全部抓取下来，这一般出现在电商公司。例如在每天将将淘宝女装的信息抓取到数据库，方便运营人员统计分析。被动式爬虫一般要求的数据量大，难度相对会高一些，几本也算是爬虫界的分水岭。

言归正传，在爬虫的时候遇到验证码怎么办？在条件不太富裕的情况下，可以选择更换 `UA` 来避免爬虫被发现的概率。作者收集了几百个 **头部UA** ，下载链接：

<a href="/code/crawler11/头部UA.txt" target="_blank">头部UA.txt</a>

抓取前筛选出一部分想要的 `UA` ，按行读取：

```
#!/usr/bin/python3.4
# -*- coding: utf-8 -*-

fileua = open("头部UA.txt")
uas = fileua.readlines()
for item in uas:
    ua = item.strip()
    print(ua)
```

每次抓取构造 `header` 的时候使用新的 `UA` ：

```
header = {
        "User-Agent": ua,
    }
```

这样被发现为爬虫的概率降低了，弹出验证码的概率也低了。

但是一般这样还是不够，如果再加上伪装 `cookie` 就更完美了，可是如何获取 `cookie` 呢？一般在伪装头部的时候会有一个字段 `host` ， 构造一个简单的头部访问 `host` ，就可以得到 `cookie` 了，再携带这个 `cookie` 访问想要抓取的网址：

```

```

<p style="text-align:center"><img  src="/img/crawler10/result1.jpg"/></p>

# 源码

<a href="/code/crawler10/crawler10.py" target="_blank">crawler10.py</a>
