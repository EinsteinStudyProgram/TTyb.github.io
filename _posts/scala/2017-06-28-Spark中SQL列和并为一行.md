---
layout: post
categories: [scala]
title: Spark中SQL列和并为一行
date: 2017-06-28
author: TTyb
desc: "在使用数据库的时候，需要将查询出来的一列按照逗号合并成一行"
---

在使用数据库的时候，需要将查询出来的一列按照逗号合并成一行。

原表名字为 `TABLE` ，表中的部分原始数据为：

```
+---------+------------------------+
| BASIC | NAME               |
+----------+------------------------+
| 1        | 有害程序事件(MI)       |
| 0        | 计算机病毒事件         |
| 0        | 蠕虫事件               |
| 0        | 特洛伊木马事件         |
+----------+------------------------+
```

查询代码为：

```
select GROUP_CONCAT(NAME SEPARATOR  ',') as NAME from TABLE where BASIC=0;
```

得到部分结果为：

```
+---------------------------------------------------------+
 | NAME               |
+---------------------------------------------------------+
| 计算机病毒事件,蠕虫事件,特洛伊木马事件         |
+---------------------------------------------------------+
```

但是在 `spark` 中没有 `GROUP_CONCAT` 命令，查找后发现命令 `concat_ws` ：

```
ResultDF.createOrReplaceTempView("BIGDATA")
val dataDF=spark.sql("select BASIC,concat_ws(',',collect_set(NAME)) as NAMES from BIGDATA group by BASIC")
```

得到结果：

```
+----------+------------------------------------------------+
| BASIC | NAMES               |
+----------+------------------------------------------------+
| 1        | 有害程序事件(MI)       |
| 0        | 计算机病毒事件,蠕虫事件,特洛伊木马事件         |
+----------+------------------------------------------------+
```

也可以用另一个方法：

```
import org.apache.spark.sql.functions._  
ResultDF.groupBy("BASIC ")  
           .agg(collect_set("NAME"))  
           .show(10,false)  
```

但是得到的结果为 `List` ：

```
+----------+------------------------------------------------+
| BASIC | NAMES               |
+----------+------------------------------------------------+
| 1        | 有害程序事件(MI)       |
| 0        | [计算机病毒事件,蠕虫事件,特洛伊木马事件]        |
+----------+------------------------------------------------+
```